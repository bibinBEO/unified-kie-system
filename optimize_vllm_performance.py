#!/usr/bin/env python3
"""
Quick vLLM performance optimization for invoice processing
"""

import requests
import json

print("ğŸš€ vLLM Performance Optimization Check")
print("=" * 50)

# Check server status
try:
    health = requests.get("http://localhost:8000/health", timeout=5)
    print(f"âœ… Server responding: {health.status_code}")
    
    if health.status_code == 200:
        try:
            data = health.json()
            print(f"âœ… GPU Available: {data.get('gpu_available', False)}")
            print(f"âœ… RAM Usage: {data.get('memory_usage', {}).get('ram_usage_mb', 0):.1f} MB")
            print(f"âœ… GPU Memory: {data.get('memory_usage', {}).get('gpu_memory_allocated_mb', 0):.1f} MB")
        except:
            print("âš ï¸  Health endpoint returned HTML instead of JSON")
            print("   This suggests an internal server error")
    
except Exception as e:
    print(f"âŒ Server check failed: {e}")

print("\nğŸ”§ Performance Issues Likely Causes:")
print("1. GPU Memory Saturation (vLLM + multiple models)")
print("2. Model loading every request (no model caching)")
print("3. Large input image processing") 
print("4. Multiple concurrent requests")

print("\nğŸ¯ Quick Fixes Needed:")
print("â€¢ Optimize GPU memory allocation")
print("â€¢ Implement request queuing") 
print("â€¢ Add image preprocessing/resizing")
print("â€¢ Configure vLLM model caching")
print("â€¢ Set processing timeouts")

print(f"\nğŸ“‹ Status: vLLM needs performance tuning for production speed")